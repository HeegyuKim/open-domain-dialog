
project: open-domain-dialog
run_name: gpt2-dialog

logger: 
  # name: wandb
  val_sample_batches: 10
  val_sample_generation_params:
    num_beams: 4
    repetition_penalty: 2.0
    no_repeat_ngram_size: 4
    max_new_tokens: 32
    eos_token_id: 375 # \n
    max_prompt_length: 224

dataset:
  train:
    shuffle: false
    use_auth_token: true
    paths:
      - heegyu/aihub_daily_conv_2022_gpt
  validation:
    use_auth_token: true
    split: train
    paths:
      - heegyu/aihub_daily_conv_2022_CRF

optimizer:
  cls: adamw
  learning_rate: 5e-5
  
trainer:
  # accelerator: cpu
  train_epochs: 3
  train_batch_size: 4
  # accumulate_grad_batches: 16
  limit_train_batches: 500
  limit_val_batches: 10

  eval_batch_size: 4
  num_sanity_val_steps: 1
  # val_check_interval: 100
  check_val_every_n_epoch: 1

model:
  plm: skt/kogpt2-base-v2
  max_seq_len: 64

tokenizer:
  add_special_tokens:
    pad_token: <pad>
    bos_token: <s>
    eos_token: </s>