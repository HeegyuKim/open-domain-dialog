{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heegyukim\\Desktop\\project\\open-domain-dialog\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odd.gpt import utils\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<|endoftext|>', 'sep_token': '\\n', 'pad_token': '<pad>'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"sep_token\": \"\\n\"\n",
    "})\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25906, 8702, 7801, 8084, 375]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.encode(\"\\n\")\n",
    "tokenizer.encode(\"안녕하세요\\n\")\n",
    "# tokenizer.tokenize(\"안녕하세요 \\n ㅋ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25906,  8702,  7801,  8084,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [36230, 36230,  8102,  8084,  9138,  6831,  7877,  7692,  7182,  9297,\n",
      "          8505,  7445,  7755,   597,  7739,   464]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 8702,  7801,  8084,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [36230,  8102,  8084,  9138,  6831,  7877,  7692,  7182,  9297,  8505,\n",
      "          7445,  7755,   597,  7739,   464,   464]])}\n",
      "['안녕하세요</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '반가 반가워요 반갑스비다 크크루삥ᄇ뽕z']\n",
      "['하세요</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '반가워요 반갑스비다 크크루삥ᄇ뽕zz']\n",
      "torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"안녕하세요\",\n",
    "    \"반가 반가워요 반갑스비다 크크루삥ㅂ뽕zzzㅋㅋㅋㅋㅋzz\"\n",
    "]\n",
    "\n",
    "out = utils.prepare_batch(\n",
    "    tokenizer,\n",
    "    texts,\n",
    "    16,\n",
    "    \"cpu\",\n",
    "    # add_bos_token=True,\n",
    "    add_eos_token=True\n",
    ")\n",
    "print(out)\n",
    "\n",
    "\n",
    "print(tokenizer.batch_decode(out[\"input_ids\"]))\n",
    "print(tokenizer.batch_decode(out[\"labels\"].masked_fill(out[\"labels\"] == -100, 3)))\n",
    "print(out[\"attention_mask\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project': 'open-domain-dialog', 'run_name': 'gpt2-dialog', 'logger': {'name': 'wandb', 'val_sample_batches': 10, 'val_sample_generate_params': {'num_beams': 4, 'repetition_penalty': 2.0, 'max_new_tokens': 32}}, 'dataset': {'train': {'shuffle': True, 'use_auth_token': True, 'paths': ['heegyu/nikl_online_conv_2022']}, 'validation': {'use_auth_token': True, 'split': 'test', 'paths': ['heegyu/aihub_daily_conv_2022_CRF']}}, 'optimizer': {'cls': 'adamw', 'learning_rate': 0.001}, 'trainer': {'train_epochs': 3, 'train_batch_size': 16, 'accumulate_grad_batches': 16, 'limit_val_batches': 200, 'eval_batch_size': 4, 'num_sanity_val_steps': 1, 'check_val_every_n_epoch': 1}, 'model': {'plm': 'skt/kogpt2-base-v2', 'max_seq_len': 512}, 'tokenizer': {'add_special_tokens': {'pad_token': '<pad>', 'bos_token': '<s>', 'eos_token': '</s>'}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odd.gpt.gpt import GPTTask\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance()\n",
    "initialize(\"../config\")\n",
    "config = compose(\"gpt\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTTask(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 75, but ``max_length`` is set to 20. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (20) must match the existing size (76) at non-singleton dimension 0.  Target sizes: [20].  Tensor sizes: [76]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m안녕하세요??\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m반가워요, 오늘은 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [\"안녕하\"],\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/open-domain-dialog/odd/gpt/gpt.py:49\u001b[0m, in \u001b[0;36mGPTTask.generate\u001b[0;34m(self, prompt, return_with_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmax_seq_len, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m dataset_utils\u001b[39m.\u001b[39mswitch_dict_tensor_device(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 49\u001b[0m generations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mX, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_with_prompt:\n\u001b[1;32m     51\u001b[0m     prompt_lens \u001b[39m=\u001b[39m X[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation_utils.py:1354\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1351\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1352\u001b[0m     )\n\u001b[1;32m   1353\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1355\u001b[0m         input_ids,\n\u001b[1;32m   1356\u001b[0m         beam_scorer,\n\u001b[1;32m   1357\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1358\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1359\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1360\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1361\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1362\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1363\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1364\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1365\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1368\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1370\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1371\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         renormalize_logits\u001b[39m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1376\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation_utils.py:2291\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2289\u001b[0m             this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m sequence_outputs \u001b[39m=\u001b[39m beam_scorer\u001b[39m.\u001b[39;49mfinalize(\n\u001b[1;32m   2292\u001b[0m     input_ids,\n\u001b[1;32m   2293\u001b[0m     beam_scores,\n\u001b[1;32m   2294\u001b[0m     next_tokens,\n\u001b[1;32m   2295\u001b[0m     next_indices,\n\u001b[1;32m   2296\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   2297\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   2298\u001b[0m     max_length\u001b[39m=\u001b[39;49mstopping_criteria\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m   2299\u001b[0m     beam_indices\u001b[39m=\u001b[39;49mbeam_indices,\n\u001b[1;32m   2300\u001b[0m )\n\u001b[1;32m   2302\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate:\n\u001b[1;32m   2303\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_scores:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation_beam_search.py:372\u001b[0m, in \u001b[0;36mBeamSearchScorer.finalize\u001b[0;34m(self, input_ids, final_beam_scores, final_beam_tokens, final_beam_indices, max_length, pad_token_id, eos_token_id, beam_indices)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[39m# fill with hypotheses and eos_token_id if the latter fits in\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39mfor\u001b[39;00m i, (hypo, best_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(best, best_indices)):\n\u001b[0;32m--> 372\u001b[0m     decoded[i, : sent_lengths[i]] \u001b[39m=\u001b[39m hypo\n\u001b[1;32m    374\u001b[0m     \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m         indices[i, : \u001b[39mlen\u001b[39m(best_idx)] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(best_idx)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (20) must match the existing size (76) at non-singleton dimension 0.  Target sizes: [20].  Tensor sizes: [76]"
     ]
    }
   ],
   "source": [
    "gpt.generate(\n",
    "    [\"안녕하세요??\", \"반가워요, 오늘은 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\"],\n",
    "    # [\"안녕하\"],\n",
    "    # max_new_tokens=32,\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 51200]) torch.Size([2, 8])\n",
      "logits\n",
      "targets\n",
      "loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.7274, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.step([\"안녕핫ㅇ\", \"방방가가가워요\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dialog'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HEEGYU~1\\AppData\\Local\\Temp/ipykernel_11756/2412739236.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m gpt.training_step(\n\u001b[0m\u001b[0;32m      2\u001b[0m     {\n\u001b[0;32m      3\u001b[0m         \u001b[1;34m\"utterances\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"안녕\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"하세요\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"반갑\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"습\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"니닼ㅋ\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     }, 0\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\heegyukim\\Desktop\\project\\open-domain-dialog\\odd\\gpt\\gpt.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dialog\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dialog'"
     ]
    }
   ],
   "source": [
    "gpt.training_step(\n",
    "    {\n",
    "        \"utterances\": [[\"안녕\", \"하세요\"], [\"반갑\", \"습\", \"니닼ㅋ\"]]\n",
    "    }, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아 오늘 뭐함</s>아 오늘 뭐함</s>', '그러게</s>그러게</s>']\n",
      "['아 오늘 뭐함</s>', '그러게</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': ['아 오늘 뭐함</s>', '그러게</s>'],\n",
       " 'response': ['아 오늘 뭐함</s>', '그러게</s>'],\n",
       " 'prediction': ['<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>',\n",
       "  '<pad><pad> <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.validation_step(\n",
    "    {\n",
    "        \"context\": [\"아 오늘 뭐함\", \"그러게\"],\n",
    "        \"response\": [\"아 오늘 뭐함\", \"그러게\"]\n",
    "    }, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 : 안ㄴ</s>2 : ㅋㅋ</s>', '1 : ??</s>2 : ㅎㅎㅎ</s>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt._join_uttrs([[\"안ㄴ\", \"ㅋㅋ\"], [\"??\", \"ㅎㅎㅎ\"]], [[1,2,],[1,2,]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
